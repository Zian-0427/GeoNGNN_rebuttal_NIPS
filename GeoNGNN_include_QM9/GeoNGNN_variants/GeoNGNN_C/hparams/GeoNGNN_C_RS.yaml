# training config
global_config:
    seed: 9999

trainer_config:
    max_epochs: 8000
    validation_interval: 1
    early_stopping_patience: 100

data_config:
    train_batch_size: 64
    val_batch_size: 64
    test_batch_size: 64
    
scheduler_config:
    RLROP_factor: 0.8
    RLROP_patience: 25
    RLROP_cooldown: 25
    RLROP_threshold: 0.001
    T_max: 30
    eta_min: 0.00002
    EXLR_gamma: 0.999
    warmup_epoch: 20
    scheduler_type: cosine

optimizer_config:
    learning_rate: 0.0002
    force_ratio: 0.999
    gradient_clip_val: 10
    ema_decay: 0.99
    weight_decay: 0.0000001


# model_config
model_config:
    rbf: nexpnorm
    max_z: 55
    rbf_trainable: false
    z_hidden_dim: 128
    ef_dim: 32
    hidden_dim: 128
    inner_layer_num: 3
    outer_layer_num: 1

    subg_cutoff: 330.0
    cutoff: 330.0
    
    activation_fn: ssp
    ablation_innerGNN: false

    extend_r: 300

    C_power: 0.5

    ablation_PE: false

    max_neighbor: 10